# ğŸ› ï¸ HÆ°á»›ng Dáº«n CÃ i Äáº·t MÃ´i TrÆ°á»ng

## ğŸ“‹ YÃªu Cáº§u Há»‡ Thá»‘ng

### Pháº§n Cá»©ng Tá»‘i Thiá»ƒu
- **CPU:** 4 cores (khuyáº¿n nghá»‹ 8 cores)
- **RAM:** 8GB (khuyáº¿n nghá»‹ 16GB)
- **á»” cá»©ng:** 50GB trá»‘ng (khuyáº¿n nghá»‹ SSD)
- **Máº¡ng:** Káº¿t ná»‘i Internet á»•n Ä‘á»‹nh

### Pháº§n Má»m
- **OS:** CentOS 7/8 hoáº·c Ubuntu 20.04+
- **Java:** JDK 8 hoáº·c 11
- **Python:** 3.7+
- **Git:** 2.x

---

## ğŸ”§ CÃ i Äáº·t Hadoop Ecosystem

### 1. CÃ i Äáº·t Java
```bash
# CentOS
sudo yum install java-11-openjdk java-11-openjdk-devel -y

# Ubuntu
sudo apt update
sudo apt install openjdk-11-jdk -y

# Kiá»ƒm tra
java -version
```

### 2. Táº¡o User Hadoop
```bash
sudo adduser hadoop
sudo usermod -aG wheel hadoop  # CentOS
# hoáº·c
sudo usermod -aG sudo hadoop   # Ubuntu

# Chuyá»ƒn sang user hadoop
su - hadoop
```

### 3. CÃ i Äáº·t Hadoop
```bash
# Download Hadoop
cd ~
wget https://downloads.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz

# Giáº£i nÃ©n
tar -xzf hadoop-3.3.6.tar.gz
mv hadoop-3.3.6 hadoop

# Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng
echo 'export HADOOP_HOME=/home/hadoop/hadoop' >> ~/.bashrc
echo 'export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop' >> ~/.bashrc
echo 'export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin' >> ~/.bashrc
source ~/.bashrc
```

### 4. Cáº¥u HÃ¬nh Hadoop

#### a) Cáº¥u hÃ¬nh `core-site.xml`
```bash
nano $HADOOP_HOME/etc/hadoop/core-site.xml
```
```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://localhost:9000</value>
    </property>
</configuration>
```

#### b) Cáº¥u hÃ¬nh `hdfs-site.xml`
```bash
nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml
```
```xml
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>1</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///home/hadoop/hadoop_data/hdfs/namenode</value>
    </property>
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///home/hadoop/hadoop_data/hdfs/datanode</value>
    </property>
</configuration>
```

#### c) Táº¡o thÆ° má»¥c lÆ°u trá»¯
```bash
mkdir -p ~/hadoop_data/hdfs/namenode
mkdir -p ~/hadoop_data/hdfs/datanode
```

#### d) Format NameNode
```bash
hdfs namenode -format
```

### 5. Khá»Ÿi Äá»™ng Hadoop
```bash
# Start HDFS
start-dfs.sh

# Kiá»ƒm tra
jps
# Output: NameNode, DataNode, SecondaryNameNode

# Kiá»ƒm tra Web UI
# http://localhost:9870
```

---

## ğŸ CÃ i Äáº·t Apache Hive

### 1. Download & CÃ i Äáº·t
```bash
cd ~
wget https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz
tar -xzf apache-hive-3.1.3-bin.tar.gz
mv apache-hive-3.1.3-bin hive

# Cáº¥u hÃ¬nh biáº¿n mÃ´i trÆ°á»ng
echo 'export HIVE_HOME=/home/hadoop/hive' >> ~/.bashrc
echo 'export PATH=$PATH:$HIVE_HOME/bin' >> ~/.bashrc
source ~/.bashrc
```

### 2. Cáº¥u HÃ¬nh Hive
```bash
cd $HIVE_HOME/conf
cp hive-default.xml.template hive-site.xml
nano hive-site.xml
```

### 3. Táº¡o ThÆ° Má»¥c HDFS cho Hive
```bash
hdfs dfs -mkdir -p /user/hive/warehouse
hdfs dfs -mkdir -p /tmp
hdfs dfs -chmod g+w /user/hive/warehouse
hdfs dfs -chmod g+w /tmp
```

### 4. Khá»Ÿi Äá»™ng Hive
```bash
# Khá»Ÿi táº¡o schema
schematool -initSchema -dbType derby

# Start Hive
hive
```

---

## ğŸ CÃ i Äáº·t Python & Dependencies

### 1. CÃ i Äáº·t Python 3
```bash
# CentOS
sudo yum install python3 python3-pip -y

# Ubuntu
sudo apt install python3 python3-pip -y
```

### 2. Táº¡o Virtual Environment
```bash
cd ~/nhom17-bigdata-analytics
python3 -m venv venv
source venv/bin/activate
```

### 3. CÃ i Äáº·t Packages
```bash
pip install --upgrade pip
pip install beautifulsoup4 requests pandas numpy selenium
```

---

## ğŸ“Š CÃ i Äáº·t Power BI (Windows)

1. Download Power BI Desktop tá»« [Microsoft Store](https://powerbi.microsoft.com/desktop/)
2. CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng Power BI Desktop
3. Import dá»¯ liá»‡u tá»« file CSV trong `data/processed_for_bi/`

---

## ğŸ“ˆ CÃ i Äáº·t Grafana (Optional)

### 1. CÃ i Äáº·t Grafana
```bash
# CentOS
sudo yum install -y https://dl.grafana.com/oss/release/grafana-10.0.0-1.x86_64.rpm
sudo systemctl start grafana-server
sudo systemctl enable grafana-server

# Ubuntu
sudo apt-get install -y software-properties-common
sudo add-apt-repository "deb https://packages.grafana.com/oss/deb stable main"
wget -q -O - https://packages.grafana.com/gpg.key | sudo apt-key add -
sudo apt-get update
sudo apt-get install grafana
sudo systemctl start grafana-server
```

### 2. Truy Cáº­p Grafana
- URL: `http://localhost:3000`
- Default login: `admin/admin`

---

## âœ… Kiá»ƒm Tra CÃ i Äáº·t

### 1. Kiá»ƒm tra HDFS
```bash
hdfs dfs -ls /
```

### 2. Kiá»ƒm tra Hive
```bash
hive -e "SHOW DATABASES;"
```

### 3. Kiá»ƒm tra Python
```bash
python --version
pip list
```

---

## ğŸ› Troubleshooting

### Lá»—i: "Connection refused" khi truy cáº­p HDFS
```bash
# Kiá»ƒm tra HDFS cÃ³ cháº¡y khÃ´ng
jps

# Náº¿u khÃ´ng cÃ³ NameNode/DataNode, restart
stop-dfs.sh
start-dfs.sh
```

### Lá»—i: Hive khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c
```bash
# XÃ³a metastore cÅ© vÃ  khá»Ÿi táº¡o láº¡i
rm -rf metastore_db/
schematool -initSchema -dbType derby
```

### Lá»—i: Permission denied trÃªn HDFS
```bash
hdfs dfs -chmod -R 777 /user/hive/warehouse
hdfs dfs -chmod -R 777 /tmp
```

---

## ğŸ“š TÃ i Liá»‡u Tham Kháº£o

- [Hadoop Official Documentation](https://hadoop.apache.org/docs/)
- [Hive Official Documentation](https://hive.apache.org/)
- [Power BI Documentation](https://docs.microsoft.com/power-bi/)
