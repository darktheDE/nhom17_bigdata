# MapReduce Jobs - H∆∞·ªõng D·∫´n

## üìã Danh S√°ch 10 Jobs

### ‚úÖ Job 01: Brand Count
- **M·ª•c ƒë√≠ch:** ƒê·∫øm s·ªë l∆∞·ª£ng laptop theo th∆∞∆°ng hi·ªáu
- **Input:** raw/laptops.csv
- **Output:** brand \t count
- **V√≠ d·ª•:** HP \t 150, Dell \t 120, Asus \t 100
- **Status:** ‚úÖ ƒê√£ c√≥ template

### ‚úÖ Job 02: Word Count  
- **M·ª•c ƒë√≠ch:** Ph√¢n t√≠ch t·ª´ kh√≥a ph·ªï bi·∫øn trong t√™n s·∫£n ph·∫©m laptop
- **Input:** raw/laptops.csv
- **Output:** keyword \t frequency
- **V√≠ d·ª•:** inspiron \t 45, ideapad \t 38, vivobook \t 32
- **Status:** ‚úÖ ƒê√£ c√≥ template

### ‚è≥ Job 03: Price Range Analysis
- **M·ª•c ƒë√≠ch:** Ph√¢n lo·∫°i laptop theo ph√¢n kh√∫c gi√°
- **Input:** raw/laptops.csv
- **Output:** price_range \t count \t avg_price
- **Ph√¢n kh√∫c:** D∆∞·ªõi 15tr (Budget), 15-25tr (Mid-range), 25-35tr (Premium), Tr√™n 35tr (High-end)

### ‚è≥ Job 04: RAM/Storage Distribution
- **M·ª•c ƒë√≠ch:** Ph√¢n t√≠ch ph√¢n b·ªë c·∫•u h√¨nh RAM v√† Storage
- **Input:** raw/laptops.csv
- **Output:** ram_storage_config \t count
- **V√≠ d·ª•:** 16GB_512GB \t 350, 8GB_256GB \t 180

### ‚è≥ Job 05: Rating by Brand
- **M·ª•c ƒë√≠ch:** T√≠nh ƒë√°nh gi√° trung b√¨nh theo th∆∞∆°ng hi·ªáu
- **Input:** raw/laptops.csv
- **Output:** brand \t avg_rating \t product_count
- **V√≠ d·ª•:** MacBook \t 4.95 \t 25, Dell \t 4.88 \t 120

### ‚è≥ Job 06: Discount Analysis
- **M·ª•c ƒë√≠ch:** Ph√¢n t√≠ch m·ª©c gi·∫£m gi√° (list_price - current_price)
- **Input:** raw/laptops.csv
- **Output:** brand \t avg_discount_percent \t max_discount
- **V√≠ d·ª•:** HP \t 11.2% \t 20%

### ‚è≥ Job 07: CPU Analysis
- **M·ª•c ƒë√≠ch:** Th·ªëng k√™ c√°c lo·∫°i CPU ph·ªï bi·∫øn (Intel i3/i5/i7, AMD R5/R7, Apple M)
- **Input:** raw/laptops.csv
- **Output:** cpu_family \t count \t avg_price
- **V√≠ d·ª•:** Intel i5 \t 280, AMD R5 \t 150, Apple M \t 45

### ‚è≥ Job 09: OS Distribution
- **M·ª•c ƒë√≠ch:** Ph√¢n b·ªë h·ªá ƒëi·ªÅu h√†nh (Windows 11 vs macOS)
- **Input:** raw/laptops.csv
- **Output:** os \t count \t market_share_percent \t avg_price
- **V√≠ d·ª•:** Windows 11 \t 850 \t 85% \t 18500000

### ‚è≥ Job 10: Price-Rating Correlation
- **M·ª•c ƒë√≠ch:** Ph√¢n t√≠ch m·ªëi t∆∞∆°ng quan gi·ªØa gi√° v√† ƒë√°nh gi√°
- **Input:** raw/laptops.csv
- **Output:** price_range \t avg_rating \t product_count
- **V√≠ d·ª•:** 25-35tr \t 4.92 \t 120

### ‚è≥ Job 08: Discount Comparison by Platform
- **M·ª•c ƒë√≠ch:** So s√°nh t·ª∑ l·ªá gi·∫£m gi√° gi·ªØa c√°c s√†n th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠
- **Input:** raw/laptops.csv
- **Output:** source_website \t avg_discount_percent \t max_discount
- **V√≠ d·ª•:** thegioididong.com \t 12.5% \t 25%, cellphones.com.vn \t 10.8% \t 22%

---

## üöÄ C√°ch Ch·∫°y MapReduce Job

### 1. Chu·∫©n B·ªã
```bash
# Upload d·ªØ li·ªáu l√™n HDFS
hdfs dfs -mkdir -p /user/bigdata/laptops/raw
hdfs dfs -put data/raw/laptops.csv /user/bigdata/laptops/raw/

# Ki·ªÉm tra
hdfs dfs -ls /user/bigdata/laptops/raw/
```

### 2. Ch·∫°y Job (V√≠ d·ª• Job 01)
```bash
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -files src/mapreduce/job01_brand_count/mapper.py,src/mapreduce/job01_brand_count/reducer.py \
  -mapper "python3 mapper.py" \
  -reducer "python3 reducer.py" \
  -input /user/bigdata/laptops/raw/laptops.csv \
  -output /user/bigdata/laptops/output/job01_brand_count
```

### 3. Xem K·∫øt Qu·∫£
```bash
# Xem output
hdfs dfs -cat /user/bigdata/laptops/output/job01_brand_count/part-00000

# Ho·∫∑c download v·ªÅ
hdfs dfs -get /user/bigdata/laptops/output/job01_brand_count/ ./output/
```

### 4. Ch·∫°y T·∫•t C·∫£ Jobs (Script)
```bash
#!/bin/bash
# run_all_jobs.sh

for i in {01..10}; do
    echo "Running Job $i..."
    
    job_dir="src/mapreduce/job${i}_*"
    output_dir="/user/bigdata/laptops/output/job${i}_*"
    
    # X√≥a output c≈© n·∫øu c√≥
    hdfs dfs -rm -r $output_dir
    
    # Ch·∫°y job
    hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
      -files ${job_dir}/mapper.py,${job_dir}/reducer.py \
      -mapper "python3 mapper.py" \
      -reducer "python3 reducer.py" \
      -input /user/bigdata/laptops/raw/laptops.csv \
      -output $output_dir
    
    echo "Job $i completed!"
done
```

---

## üìù Template Mapper/Reducer

### Mapper Template
```python
#!/usr/bin/env python3
import sys

def mapper():
    for line in sys.stdin:
        line = line.strip()
        
        # B·ªè qua header
        if line.startswith('id,'):
            continue
        
        try:
            fields = line.split(',')
            # X·ª≠ l√Ω d·ªØ li·ªáu
            # ...
            # Emit: key \t value
            print(f"{key}\t{value}")
        except:
            continue

if __name__ == "__main__":
    mapper()
```

### Reducer Template
```python
#!/usr/bin/env python3
import sys
from collections import defaultdict

def reducer():
    data = defaultdict(int)  # ho·∫∑c list, dict t√πy logic
    
    for line in sys.stdin:
        line = line.strip()
        try:
            key, value = line.split('\t')
            # Aggregate data
            # ...
        except:
            continue
    
    # Emit results
    for key, result in data.items():
        print(f"{key}\t{result}")

if __name__ == "__main__":
    reducer()
```

---

## üêõ Troubleshooting

### L·ªói: "No such file or directory"
- Ki·ªÉm tra ƒë∆∞·ªùng d·∫´n HDFS: `hdfs dfs -ls /user/bigdata/laptops/`

### L·ªói: "Output directory already exists"
- X√≥a output c≈©: `hdfs dfs -rm -r /user/bigdata/laptops/output/jobXX_*`

### L·ªói: "Permission denied"
- C·∫•p quy·ªÅn: `hdfs dfs -chmod -R 777 /user/bigdata/`

---

## üìä K·∫øt Qu·∫£ Mong ƒê·ª£i

Sau khi ch·∫°y xong 10 jobs, b·∫°n s·∫Ω c√≥:
- 10 th∆∞ m·ª•c output tr√™n HDFS
- M·ªói th∆∞ m·ª•c ch·ª©a file part-00000 v·ªõi k·∫øt qu·∫£ ƒë√£ t·ªïng h·ª£p
- Data Analyst (Anh Th·ªãnh) s·∫Ω d√πng Hive ƒë·ªÉ query c√°c k·∫øt qu·∫£ n√†y
