# üìä ·ª®NG D·ª§NG HADOOP TH·ª∞C HI·ªÜN X√ÇY D·ª∞NG H·ªÜ TH·ªêNG PH√ÇN T√çCH C·∫†NH TRANH TH·ªä TR∆Ø·ªúNG D·ª∞A TR√äN D·ªÆ LI·ªÜU GI√Å V√Ä KHUY·∫æN M√ÉI S·∫¢N PH·∫®M LAPTOP

## üéØ Gi·ªõi Thi·ªáu D·ª± √Ån

**ƒê·ªÅ t√†i:** ·ª®ng d·ª•ng Hadoop th·ª±c hi·ªán x√¢y d·ª±ng h·ªá th·ªëng ph√¢n t√≠ch c·∫°nh tranh th·ªã tr∆∞·ªùng d·ª±a tr√™n d·ªØ li·ªáu gi√° v√† khuy·∫øn m√£i s·∫£n ph·∫©m Laptop t·ª´ hai trang web: [www.thegioididong.com](http://www.thegioididong.com) v√† [www.cellphones.com.vn](http://www.cellphones.com.vn).

### üéØ M·ª•c ti√™u ch√≠nh

X√¢y d·ª±ng m·ªôt h·ªá th·ªëng Big Data ho√†n ch·ªânh c√≥ kh·∫£ nƒÉng:
- **Thu th·∫≠p, l∆∞u tr·ªØ, v√† x·ª≠ l√Ω** d·ªØ li·ªáu v·ªÅ gi√° v√† c√°c ch∆∞∆°ng tr√¨nh khuy·∫øn m√£i c·ªßa s·∫£n ph·∫©m laptop t·ª´ hai nh√† b√°n l·∫ª l·ªõn
- **Ph√¢n t√≠ch d·ªØ li·ªáu** ƒë·ªÉ cung c·∫•p c√°i nh√¨n s√¢u s·∫Øc v·ªÅ chi·∫øn l∆∞·ª£c c·∫°nh tranh tr√™n th·ªã tr∆∞·ªùng
- **Cung c·∫•p giao di·ªán tr·ª±c quan** ƒë·ªÉ ng∆∞·ªùi d√πng c√≥ th·ªÉ th·ª±c hi·ªán truy v·∫•n v√† xem k·∫øt qu·∫£ ph√¢n t√≠ch

### üë• ƒê·ªëi t∆∞·ª£ng h∆∞·ªõng ƒë·∫øn
- **Ng∆∞·ªùi ti√™u d√πng:** D·ªÖ d√†ng t√¨m ki·∫øm v√† so s√°nh ƒë·ªÉ ch·ªçn ƒë∆∞·ª£c s·∫£n ph·∫©m c√≥ gi√° v√† ∆∞u ƒë√£i t·ªët nh·∫•t
- **Doanh nghi·ªáp:** Theo d√µi, ph√¢n t√≠ch chi·∫øn l∆∞·ª£c c·ªßa ƒë·ªëi th·ªß ƒë·ªÉ ƒë∆∞a ra c√°c quy·∫øt ƒë·ªãnh kinh doanh hi·ªáu qu·∫£

## üë• Th√†nh Vi√™n Nh√≥m

| H·ªç T√™n | Vai Tr√≤ | Nhi·ªám V·ª• Ch√≠nh |
|--------|---------|----------------|
| Phan Tr·ªçng Ph√∫ | Team H·∫° t·∫ßng & D·ªØ li·ªáu | Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Hadoop, c·∫•u h√¨nh h·ªá th·ªëng |
| Phan Tr·ªçng Qu√≠ | Team H·∫° t·∫ßng & D·ªØ li·ªáu | Web Scraping, L√†m s·∫°ch d·ªØ li·ªáu, ETL Pipeline |
| ƒê·ªó Ki·∫øn H∆∞ng | Big Data Developer | L·∫≠p tr√¨nh 10 ch∆∞∆°ng tr√¨nh MapReduce (Python) |
| Ph·∫°m VƒÉn Th·ªãnh | Data Analyst | Ph√¢n t√≠ch d·ªØ li·ªáu v·ªõi Hive/HiveQL v√† Apache Drill |
| Nguy·ªÖn VƒÉn Quang Duy | BI Developer | Tr·ª±c quan h√≥a v·ªõi Apache Zeppelin v√† PySpark |

## üõ†Ô∏è C√¥ng Ngh·ªá S·ª≠ D·ª•ng

### H·ªá sinh th√°i Hadoop & Big Data
- **L∆∞u tr·ªØ ph√¢n t√°n:** Hadoop HDFS, YARN
- **X·ª≠ l√Ω d·ªØ li·ªáu:** Apache Hadoop MapReduce
- **ETL & Transformation:** Apache Pig, Apache Sqoop
- **Truy v·∫•n d·ªØ li·ªáu:** Apache Hive (HiveQL), Apache Drill, Apache Phoenix
- **C∆° s·ªü d·ªØ li·ªáu:** HBase (NoSQL), MySQL (RDBMS)
- **Ph√¢n t√≠ch & T∆∞∆°ng t√°c:** Apache Spark (PySpark), Apache Zeppelin
- **ƒêi·ªÅu ph·ªëi:** Apache Zookeeper

### M√¥i tr∆∞·ªùng ph√°t tri·ªÉn
- **H·ªá ƒëi·ªÅu h√†nh:** Ubuntu tr√™n WSL 2
- **Ng√¥n ng·ªØ l·∫≠p tr√¨nh:** Python (Web Scraping, PySpark), Java (MapReduce)
- **Qu·∫£n l√Ω m√£ ngu·ªìn:** GitHub


## üìÅ C·∫•u Tr√∫c D·ª± √Ån

```
nhom17_bigdata/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                           # D·ªØ li·ªáu th√¥ t·ª´ web scraping
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cellphones_raw_data.csv    # D·ªØ li·ªáu s·∫£n ph·∫©m CellphoneS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cellphones_promotions_nosql.json  # Khuy·∫øn m√£i CellphoneS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tgdd_raw_data.csv          # D·ªØ li·ªáu s·∫£n ph·∫©m TGDƒê
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tgdd_promotions_nosql.json # Khuy·∫øn m√£i TGDƒê
‚îÇ   ‚îú‚îÄ‚îÄ sample/                        # D·ªØ li·ªáu m·∫´u
‚îÇ   ‚îî‚îÄ‚îÄ processed_for_bi/              # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω cho BI
‚îú‚îÄ‚îÄ docs/                              # T√†i li·ªáu d·ª± √°n
‚îÇ   ‚îú‚îÄ‚îÄ ABOUT_PROJECT.md               # T·ªïng quan d·ª± √°n
‚îÇ   ‚îú‚îÄ‚îÄ ABOUT_DATA.md                  # M√¥ t·∫£ chi ti·∫øt d·ªØ li·ªáu
‚îÇ   ‚îî‚îÄ‚îÄ ABOUT_PERSONAL_ROLE.md         # Ph√¢n c√¥ng nhi·ªám v·ª•
‚îú‚îÄ‚îÄ reports/                           # B√°o c√°o, slide, video demo
‚îÇ   ‚îî‚îÄ‚îÄ video/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ scraping/                      # Script thu th·∫≠p d·ªØ li·ªáu
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tgdd_scraper.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cellphones_scraper.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sql_config.sql
‚îÇ   ‚îú‚îÄ‚îÄ mapreduce/                     # 10 MapReduce jobs (Python)
‚îÇ   ‚îî‚îÄ‚îÄ hive/                          # HiveQL scripts
‚îî‚îÄ‚îÄ README.md
```

## üìä D·ªØ Li·ªáu D·ª± √Ån

### Ngu·ªìn d·ªØ li·ªáu
- **Th·∫ø Gi·ªõi Di ƒê·ªông:** [www.thegioididong.com](https://www.thegioididong.com)
- **CellphoneS:** [www.cellphones.com.vn](https://cellphones.com.vn)
- **Ph∆∞∆°ng ph√°p:** Web Crawling (t·ª± ƒë·ªông)
- **Quy m√¥:** Kho·∫£ng **1,200 s·∫£n ph·∫©m laptop**

### C·∫•u tr√∫c d·ªØ li·ªáu

#### 1. D·ªØ li·ªáu c√≥ c·∫•u tr√∫c (CSV) - Th√¥ng tin s·∫£n ph·∫©m
- `id`: M√£ ƒë·ªãnh danh duy nh·∫•t
- `product_name`: T√™n s·∫£n ph·∫©m
- `current_price_raw`: Gi√° b√°n hi·ªán t·∫°i
- `list_price_raw`: Gi√° ni√™m y·∫øt
- `raw_specs_string`: Chu·ªói m√¥ t·∫£ c·∫•u h√¨nh
- `product_url`: URL s·∫£n ph·∫©m

**ƒê·∫∑c ƒëi·ªÉm:**
- **CellphoneS:** Gi√° d·∫°ng chu·ªói ("22.990.000ƒë") - c·∫ßn l√†m s·∫°ch
- **TGDƒê:** Gi√° d·∫°ng s·ªë (13190000.0) - s·∫µn s√†ng x·ª≠ l√Ω

#### 2. D·ªØ li·ªáu phi c·∫•u tr√∫c (JSON) - Th√¥ng tin khuy·∫øn m√£i
```json
{
  "product_id": "xxx",
  "product_url": "...",
  "promotions": [
    "Tr·∫£ g√≥p 0% l√£i su·∫•t",
    "T·∫∑ng Balo",
    "Gi·∫£m ngay 700,000ƒë"
  ]
}
```

**ƒê·∫∑c ƒëi·ªÉm:** VƒÉn b·∫£n t·ª± do, c·∫ßn x·ª≠ l√Ω NLP ƒë·ªÉ ph√¢n lo·∫°i

## üîÑ Ki·∫øn Tr√∫c H·ªá Th·ªëng v√† Lu·ªìng X·ª≠ L√Ω ETL

### Pipeline x·ª≠ l√Ω d·ªØ li·ªáu

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Web Scraping   ‚îÇ ‚îÄ‚îÄ> Python Scripts
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Raw Data      ‚îÇ ‚îÄ‚îÄ> CSV + JSON Files
‚îÇ   (Local)       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   HDFS          ‚îÇ ‚îÄ‚îÄ> Hadoop Distributed File System
‚îÇ /raw_data/      ‚îÇ     - /products_csv/
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     - /promotions_json/
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Apache Pig     ‚îÇ ‚îÄ‚îÄ> ETL & Data Transformation
‚îÇ  (ETL Layer)    ‚îÇ     - L√†m s·∫°ch d·ªØ li·ªáu CellphoneS
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     - Chu·∫©n h√≥a schema
         ‚îÇ              - H·ª£p nh·∫•t (UNION) d·ªØ li·ªáu
         ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ         ‚îÇ
    ‚ñº         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MySQL ‚îÇ  ‚îÇ HBase ‚îÇ ‚îÄ‚îÄ> Data Warehouses
‚îÇ(RDBMS)‚îÇ  ‚îÇ(NoSQL)‚îÇ     - MySQL: D·ªØ li·ªáu c√≥ c·∫•u tr√∫c
‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò     - HBase: D·ªØ li·ªáu phi c·∫•u tr√∫c
    ‚îÇ          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Analysis       ‚îÇ ‚îÄ‚îÄ> MapReduce, Hive, Drill, Spark
‚îÇ  Layer          ‚îÇ     - 10 MapReduce Jobs
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     - HiveQL Queries
         ‚îÇ              - PySpark Analytics
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Visualization   ‚îÇ ‚îÄ‚îÄ> Apache Zeppelin
‚îÇ & Reporting     ‚îÇ     - Interactive Notebooks
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     - Charts & Dashboards
```

### Chi ti·∫øt t·ª´ng b∆∞·ªõc

**B∆∞·ªõc 1: Ingestion (Thu th·∫≠p v√† L∆∞u tr·ªØ)**
- ƒê∆∞a 4 file d·ªØ li·ªáu th√¥ l√™n **HDFS**
- Ph√¢n lo·∫°i v√†o th∆∞ m·ª•c `/raw_data/products_csv` v√† `/raw_data/promotions_json`

**B∆∞·ªõc 2: Transformation (X·ª≠ l√Ω v√† L√†m s·∫°ch)**
- **Apache Pig** th·ª±c hi·ªán:
  - L√†m s·∫°ch gi√° CellphoneS: "22.990.000ƒë" ‚Üí 22990000
  - Th√™m c·ªôt `source_brand` (cellphones/tgdd)
  - UNION d·ªØ li·ªáu t·ª´ 2 ngu·ªìn
  - Chu·∫©n h√≥a d·ªØ li·ªáu JSON khuy·∫øn m√£i

**B∆∞·ªõc 3: Loading (L∆∞u v√†o Kho d·ªØ li·ªáu)**
- **Apache Sqoop:** Import d·ªØ li·ªáu c√≥ c·∫•u tr√∫c t·ª´ HDFS ‚Üí MySQL
- **HBase:** L∆∞u tr·ªØ d·ªØ li·ªáu khuy·∫øn m√£i (NoSQL) cho truy xu·∫•t nhanh

**B∆∞·ªõc 4: Analysis (Ph√¢n t√≠ch)**
- **MapReduce (Java/Python):** C√°c ph√¢n t√≠ch ph·ª©c t·∫°p
- **Apache Hive:** Truy v·∫•n SQL-like tr√™n HDFS
- **Apache Drill:** Truy v·∫•n ƒëa ngu·ªìn (MySQL + HBase)
- **Apache Spark (PySpark):** Ph√¢n t√≠ch t∆∞∆°ng t√°c

**B∆∞·ªõc 5: Presentation (Tr·ª±c quan h√≥a)**
- **Apache Zeppelin:** Notebook t∆∞∆°ng t√°c
- **GUI Web:** Giao di·ªán ng∆∞·ªùi d√πng cu·ªëi




## üöÄ H∆∞·ªõng D·∫´n Ch·∫°y D·ª± √Ån

### B∆∞·ªõc 1: Thu th·∫≠p d·ªØ li·ªáu (Web Scraping)
```bash
cd src/scraping
python tgdd_scraper.py
python cellphones_scraper.py
```

### B∆∞·ªõc 2: ƒê∆∞a d·ªØ li·ªáu l√™n HDFS (Ingestion)
```bash
# T·∫°o c·∫•u tr√∫c th∆∞ m·ª•c tr√™n HDFS
hdfs dfs -mkdir -p /raw_data/products_csv
hdfs dfs -mkdir -p /raw_data/promotions_json

# Upload d·ªØ li·ªáu s·∫£n ph·∫©m (CSV)
hdfs dfs -put data/raw/cellphones_raw_data.csv /raw_data/products_csv/
hdfs dfs -put data/raw/tgdd_raw_data.csv /raw_data/products_csv/

# Upload d·ªØ li·ªáu khuy·∫øn m√£i (JSON)
hdfs dfs -put data/raw/cellphones_promotions_nosql.json /raw_data/promotions_json/
hdfs dfs -put data/raw/tgdd_promotions_nosql.json /raw_data/promotions_json/
```

### B∆∞·ªõc 3: X·ª≠ l√Ω d·ªØ li·ªáu v·ªõi Apache Pig (ETL)
```bash
# Ch·∫°y Pig script ƒë·ªÉ l√†m s·∫°ch v√† h·ª£p nh·∫•t d·ªØ li·ªáu
pig -x mapreduce src/pig/mysql_brand_etl.pig

# Pig s·∫Ω th·ª±c hi·ªán:
# - L√†m s·∫°ch gi√° CellphoneS
# - Th√™m c·ªôt source_brand
# - UNION d·ªØ li·ªáu t·ª´ 2 ngu·ªìn
# - L∆∞u v√†o th∆∞ m·ª•c t·∫°m tr√™n HDFS
```

### B∆∞·ªõc 4: Import v√†o MySQL v·ªõi Sqoop
```bash
# Import d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω t·ª´ HDFS v√†o MySQL
sqoop import \
  --connect jdbc:mysql://localhost:3306/bigdata_db \
  --username root \
  --password your_password \
  --table products \
  --target-dir /processed_data/products \
  --m 1
```

### B∆∞·ªõc 5: L∆∞u d·ªØ li·ªáu JSON v√†o HBase
```bash
# Kh·ªüi ƒë·ªông HBase shell
hbase shell

# T·∫°o b·∫£ng
create 'promotions', 'promo_details'

# Import d·ªØ li·ªáu (s·ª≠ d·ª•ng script Python ho·∫∑c Pig)
pig -x mapreduce src/pig/hbase_promotions_import.pig
```

### B∆∞·ªõc 6: Ch·∫°y MapReduce Jobs (Python Streaming)
```bash
# Job 01: T√≠nh gi√° b√°n trung b√¨nh theo h√£ng
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -mapper "python3 mapper.py" \
  -reducer "python3 reducer.py" \
  -input /raw_data/products_csv/* \
  -output /output/job01_avg_price_by_brand \
  -file src/mapreduce/job01_avg_price/mapper.py \
  -file src/mapreduce/job01_avg_price/reducer.py

# Job 02: T√≠nh t·ª∑ l·ªá gi·∫£m gi√° trung b√¨nh
hadoop jar $HADOOP_HOME/share/hadoop/tools/lib/hadoop-streaming-*.jar \
  -mapper "python3 mapper.py" \
  -reducer "python3 reducer.py" \
  -input /raw_data/products_csv/* \
  -output /output/job02_avg_discount \
  -file src/mapreduce/job02_avg_discount/mapper.py \
  -file src/mapreduce/job02_avg_discount/reducer.py

# T∆∞∆°ng t·ª± cho 8 jobs c√≤n l·∫°i...
```

### B∆∞·ªõc 7: Ph√¢n t√≠ch v·ªõi Hive
```bash
# Kh·ªüi ƒë·ªông Hive
hive

# T·∫°o b·∫£ng t·ª´ d·ªØ li·ªáu tr√™n HDFS
CREATE EXTERNAL TABLE products (
  id STRING,
  product_name STRING,
  current_price DOUBLE,
  list_price DOUBLE,
  specs STRING,
  url STRING,
  source_brand STRING
)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
STORED AS TEXTFILE
LOCATION '/processed_data/products';

# Ch·∫°y c√°c truy v·∫•n ph√¢n t√≠ch
SELECT source_brand, AVG(current_price) as avg_price
FROM products
GROUP BY source_brand;
```

### B∆∞·ªõc 8: Ph√¢n t√≠ch v·ªõi Apache Drill
```bash
# Kh·ªüi ƒë·ªông Drill
drill-embedded

# Truy v·∫•n k·∫øt h·ª£p MySQL v√† HBase
SELECT p.product_name, p.current_price, h.promotions
FROM mysql.bigdata_db.products p
JOIN hbase.promotions h
ON p.id = h.product_id
WHERE p.current_price < 20000000;
```

### B∆∞·ªõc 9: Ph√¢n t√≠ch t∆∞∆°ng t√°c v·ªõi Zeppelin
1. M·ªü tr√¨nh duy·ªát v√† truy c·∫≠p: `http://localhost:8080`
2. T·∫°o notebook m·ªõi
3. S·ª≠ d·ª•ng PySpark ƒë·ªÉ ph√¢n t√≠ch:

```python
# ƒê·ªçc d·ªØ li·ªáu t·ª´ HDFS
df = spark.read.csv("/processed_data/products", header=True, inferSchema=True)

# Ph√¢n t√≠ch
df.groupBy("source_brand").agg({"current_price": "avg"}).show()

# Tr·ª±c quan h√≥a
%sql
SELECT source_brand, AVG(current_price) as avg_price
FROM products
GROUP BY source_brand
```
---

## üéØ C√°c Ph√¢n T√≠ch Ch√≠nh v·ªõi Hive/Drill

### Ph√¢n t√≠ch th·ªã tr∆∞·ªùng
- Th·ªã ph·∫ßn theo th∆∞∆°ng hi·ªáu (HP, Dell, Asus, Lenovo, Acer, MacBook)
- Chi·∫øn l∆∞·ª£c gi√° v√† khuy·∫øn m√£i theo t·ª´ng nh√† b√°n l·∫ª
- Xu h∆∞·ªõng c·∫•u h√¨nh ph·ªï bi·∫øn (RAM, Storage, CPU)

### Ph√¢n t√≠ch s·∫£n ph·∫©m
- Ph√¢n kh√∫c s·∫£n ph·∫©m theo m√†n h√¨nh v√† ƒë·ªô ph√¢n gi·∫£i
- Ph√¢n b·ªë h·ªá ƒëi·ªÅu h√†nh (Windows 11, macOS)
- M·ªëi t∆∞∆°ng quan gi√° - c·∫•u h√¨nh

### So s√°nh c·∫°nh tranh
- So s√°nh gi√° c√πng s·∫£n ph·∫©m gi·ªØa 2 ngu·ªìn
- Chi·∫øn l∆∞·ª£c khuy·∫øn m√£i TGDƒê vs CellphoneS
- Ph√¢n t√≠ch gap v·ªÅ gi√° v√† d·ªãch v·ª•

## üìà K·∫øt Qu·∫£ Mong ƒê·ª£i

### Cho ng∆∞·ªùi ti√™u d√πng
- ‚úÖ T√¨m ki·∫øm s·∫£n ph·∫©m c√≥ gi√° t·ªët nh·∫•t
- ‚úÖ So s√°nh khuy·∫øn m√£i gi·ªØa c√°c c·ª≠a h√†ng
- ‚úÖ X√°c ƒë·ªãnh th·ªùi ƒëi·ªÉm mua h√†ng t·ªëi ∆∞u

### Cho doanh nghi·ªáp
- ‚úÖ Gi√°m s√°t chi·∫øn l∆∞·ª£c gi√° c·ªßa ƒë·ªëi th·ªß
- ‚úÖ Ph√¢n t√≠ch xu h∆∞·ªõng th·ªã tr∆∞·ªùng
- ‚úÖ ƒê∆∞a ra quy·∫øt ƒë·ªãnh kinh doanh d·ª±a tr√™n d·ªØ li·ªáu

## ü§ù ƒê√≥ng G√≥p

D·ª± √°n n√†y ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Nh√≥m 17 - M√¥n Big Data (BDES333877)
- **Tr∆∞·ªùng:** ƒê·∫°i h·ªçc S∆∞ ph·∫°m K·ªπ thu·∫≠t TP.HCM (HCMUTE)
- **H·ªçc k·ª≥:** HK5 (2024-2025)

## üìß Li√™n H·ªá

N·∫øu c√≥ b·∫•t k·ª≥ c√¢u h·ªèi n√†o, vui l√≤ng li√™n h·ªá qua:
- **Repository:** [darktheDE/nhom17_bigdata](https://github.com/darktheDE/nhom17_bigdata)
---

**¬© 2025 Nh√≥m 17 - Big Data Analytics Project**


## üìπ Video Demo

Link video demo: [Xem t·∫°i ƒë√¢y](reports/video/link_demo.txt)

## üìÑ License

D·ª± √°n n√†y ƒë∆∞·ª£c ph√°t tri·ªÉn cho m√¥n h·ªçc Nh·∫≠p m√¥n D·ªØ Li·ªáu L·ªõn - HCMUTE.

---
**L∆∞u √Ω:** File d·ªØ li·ªáu l·ªõn kh√¥ng ƒë∆∞·ª£c commit l√™n GitHub. Ch·ªâ file m·∫´u `laptops_sample.csv` ƒë∆∞·ª£c ƒë∆∞a v√†o repository.
